**Neural Network with Batch Normalization and Dropout
**

This repository contains the implementation of a simple neural network model with batch normalization and dropout layers for regularizing and improving the training of the model.

Network Architecture

The architecture consists of the following layers:

Fully connected layer with 
2
8
2
28 
2
  input units and 100 output units.
Batch normalization layer for the 100 units.
ReLU activation function.
Dropout layer with a dropout rate of 0.5.
Fully connected layer with 100 input units and 50 output units.
Batch normalization layer for the 50 units.
ReLU activation function.
Dropout layer with a dropout rate of 0.5.
Fully connected layer with 50 input units and 10 output units
